---
title: "hw8_gr5206"
author: "Tianchen Wang / tw2665"
date: "2018/11/30"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## HW8

## 1
Accoring to the correaltion matrix, I am not able to pick out the three actual relavant variables, since 1,2,3,4,6 all have relatively large correlation to y.
```{r}
n <- 100
p <- 10
s <- 3
set.seed(0)
x <- matrix(rnorm(n*p), n, p)
b <- c(-0.7, 0.7, 1, rep(0, p-s))
y <- x %*% b + rt(n, df=2)
cor(y, x)
```

## 2
```{r}
library(ggplot2)
xaxis <- seq(-5,5,0.1)
df <- data.frame(x = rep(xaxis, 2), y = c(dnorm(xaxis), dt(xaxis, df = 3)))
df$flag <- rep(c(1,0), each = length(xaxis))

ggplot(df, aes(x = x, y = y, 
           color = as.factor(flag), group = as.factor(flag)))+
  geom_line() +
  xlab("X") +
  ylab("Probability") +
  scale_color_manual(name = "Density", values = c("red", "blue"), labels = c("Normal","T")) + 
  theme_bw()

```

## 3
```{r}
# using default cut off c = 1
psi <- function(r, c = 1) {
  return(ifelse(r^2 > c^2, 2*c*abs(r) - c^2, r^2))
}

huber.loss <- function(beta, ...){
  y_hat <- apply(x, 1, function(x) sum(x*beta))
  r <- y_hat - y
  sum(psi(r, ...))
}
```

## 4
```{r}
library(numDeriv)

grad.descent <- function(f, x0, max.iter=200, step.size=0.001, stop.deriv=0.1, ...){
  n <- length(x0)
  xmat <- matrix(0, nrow = n, ncol = max.iter)
  xmat[, 1] <- x0
  
  for(k in 2:max.iter){
    grad.cur <- grad(f, xmat[, k-1], ...)
    
    if(all(abs(grad.cur) < stop.deriv)){
      k <- k-1
      break
    }
    
    xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
  }
  xmat <- xmat[ ,1:k]
  
  return(list(x = xmat[,k], xmat = xmat, k = k))
}

gd <- grad.descent(huber.loss, x0 = rep(0, p))
cat("Number of iterations till converge ", gd$k, '\n')
cat("Final estimate of coefficients \n", gd$x)

```

## 5
Based on the Huber Loss plot, the loss value decreased super fast during the first 20 iterations, and became slower and slower after.
```{r}
obj <- apply(gd$xmat, 2, huber.loss)

plot(1:127, obj, type = 'l', lty = 1, 
     main = "Huber Loss", xlab = "iterations", ylab = "loss")
```

## 6
The loss value was vibrating heaviliy and didn't converge in the 200 iterations.
Since we were using such a big step size to run the gradient descent, it is likly that our loss would go up and down when it got close to the correct estimation.
```{r}
gd.2 <- grad.descent(huber.loss, x0 = rep(0, p), step.size = 0.1)
gd.2$k
gd.2$x
obj.2 <- apply(gd.2$xmat[,151:200], 2, huber.loss)
plot(151:200, obj.2, type = 'l', lty = 1, 
     main = "Huber Loss", xlab = "iterations", ylab = "loss")
```

## 7
```{r}
sparse.grad.descent <- function(
  f, x0, max.iter=200, step.size=0.001, stop.deriv=0.1, threshold=0.05, ...){
  
  n <- length(x0)
  xmat <- matrix(0, nrow = n, ncol = max.iter)
  xmat[, 1] <- x0
  
  for(k in 2:max.iter){
    grad.cur <- grad(f, xmat[, k-1], ...)
    
    if(all(abs(grad.cur) < stop.deriv)){
      k <- k-1
      break
    }
    xmat[ ,k] <- xmat[ ,k-1] - step.size * grad.cur
    xmat[ ,k] <- ifelse(abs(xmat[ ,k]) > threshold, xmat[ ,k], 0)
  }
  xmat <- xmat[ ,1:k]
  
  return(list(x = xmat[,k], xmat = xmat, k = k))
}

gd.sparse <- sparse.grad.descent(huber.loss, x0 = rep(0, p))
cat("Final estimation of coefficients \n",gd.sparse$x)

```

## 8
The sparsed gradient descent has the best(smallest) MSE among the three estimations.
```{r}
mse <- function(x1, x2){
  return(mean((x1 - x2)^2))
}
fit <- lm(y ~ 0 + x )
fit$coefficients

cat('Linear fit model coefficients ', mse(fit$coefficients, b), '\n')
cat("Final estimation of gd ", mse(gd$x, b), '\n')
cat("Final estimation of sparsed gd ", mse(gd.sparse$x, b), '\n')

```

## 9
```{r}
set.seed(10)
y = x %*% b + rt(n, df=2)

gd.new <- grad.descent(huber.loss, rep(0, p))
gd.2.new <- grad.descent(huber.loss, rep(0, p), step.size = 0.1)
gd.sparse.new <- sparse.grad.descent(huber.loss, rep(0, p))
cat("New estimation of gd ", gd.new$x, '\n')
cat("New estimation of sparsed gd ", gd.sparse.new$x, '\n')

cat("Final estimation of gd ", mse(gd.new$x, b), '\n')
cat("Final estimation of sparsed gd ", mse(gd.sparse.new$x, b), '\n')
```

## 10
```{r}
gd.loop.mse <- numeric(10)
gd.sparse.loop.mse <- numeric(10)

for(i in 1:10){
  y = x %*% b + rt(n, df=2)
  gd.loop <- grad.descent(huber.loss, rep(0, p))
  gd.sparse.loop <- sparse.grad.descent(huber.loss, rep(0, p))
  gd.loop.mse[i] <- mse(gd.loop$x, b)
  gd.sparse.loop.mse[i] <- mse(gd.sparse.loop$x, b)
}
mean(gd.loop.mse)
mean(gd.sparse.loop.mse)
min(gd.loop.mse)
min(gd.sparse.loop.mse)
```

